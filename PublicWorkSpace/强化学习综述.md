<!--
 * @Description: Review of reinforcement learning
 * @version: V1.0
 * @Author: lesheng
 * @Date: 2021-04-15 21:52:41
 * @LastEditors: lesheng
 * @LastEditTime: 2021-04-16 14:49:08
-->

# 强化学习综述

- [强化学习综述](#强化学习综述)
  - [定义](#定义)
  - [强化学习中的核心概念和术语](#强化学习中的核心概念和术语)
    - [**状态和观测**](#状态和观测)
    - [**动作空间**](#动作空间)
    - [**策略**](#策略)
    - [**轨迹（Trajectories）**](#轨迹trajectories)
    - [**奖励**](#奖励)
  - [强化学习算法概述](#强化学习算法概述)
    - [**基于值函数的深度强化学习**](#基于值函数的深度强化学习)
    - [**基于策略梯度的深度强化学习**](#基于策略梯度的深度强化学习)
    - [**基于搜索与监督的深度强化学习**](#基于搜索与监督的深度强化学习)
    - [**分层深度强化学习**](#分层深度强化学习)
    - [**多任务迁移深度强化学习**](#多任务迁移深度强化学习)
    - [**多 agent 深度强化学习**](#多-agent-深度强化学习)
    - [**基于记忆与推理的深度强化学习**](#基于记忆与推理的深度强化学习)
    - [**深度强化学习中的探索与利用**](#深度强化学习中的探索与利用)
  - [强化学习的应用](#强化学习的应用)
  - [强化学习发展简史](#强化学习发展简史)
    - [**时间线**](#时间线)
  - [参考](#参考)

## 定义

总的来说，强化学习是关于智能体以及它们如何通过试错来学习的研究。 它确定了通过奖励或惩罚智能体的动作从而使它未来更容易重复或者放弃某一动作的思想。其在[维基百科中的定义](https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)如下

>强化学习（英语：Reinforcement learning，简称RL）是机器学习中 的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。其关注点在于寻找探索（对未知领域的）和利用（对已有知识的）的平衡。


## 强化学习中的核心概念和术语

智能体与环境的交互循环  
![智能体与环境的交互循环](https://spinningup.qiwihui.com/zh_CN/latest/_images/rl_diagram_transparent_bg.png)

强化学习的主要角色是**智能体** 和 **环境**，环境是智能体存在和交互的世界。智能体在每一步的交互中，都会获得对于所处环境状态的观测，然后决定下一步要执行的动作。环境会因为智能体对它的动作而改变，也可能自己改变。  

智能体也会从环境中感知到**奖励**信号，一个表明当前状态好坏的数字。智能体的目标是最大化累计奖励，也就是**回报**。强化学习就是智能体通过学习行为来实现目标的方法。  

为了更具体地讨论强化的作用，我们还需要了解下面这些术语。

### **状态和观测**

一个**状态** *s* 是一个关于这个世界状态的完整描述。这个世界除了状态以外没有别的信息。**观测** *o* 是对于一个状态的部分描述。

在深度强化学习中，我们一般用向量、矩阵或更高阶的张量 表示状态和观测。比如说，视觉上的观测可以用其像素值的RGB矩阵表示；机器人的状态可以通过关节角度和速度来表示。

如果智能体观测到环境的全部状态，我们通常说环境是被**全面观测**的。如果智能体只能观测到一部分，我们称之为被**部分观测**。

### **动作空间**

不同的环境允许不同的动作。所有给定环境中有效动作的集合称之为**动作空间**。有些环境，比如说Atari游戏和围棋，属于**离散动作空间**，这种情况下智能体只能采取有限的动作。 其他的一些环境，比如智能体在物理世界中控制机器人，属于**连续动作空间**。在连续动作空间中，动作是实数向量。

### **策略**

策略是智能体用于决定下一步执行什么动作的规则。可以是**确定性策略**，也可以是**随机策略**。因为策略本质上就是智能体的大脑，所以很多时候“策略”和“智能体”这两个名词经常互换，例如我们会说：“策略的目的是最大化奖励”。  
在深度强化学习中，我们处理的是 参数化的策略：策略的输出依赖于一系列计算函数，而这些函数又依赖于参数（例如神经网络的权重和偏差），所以我们可以通过一些优化算法改变智能体的的行为。

### **轨迹（Trajectories）**

轨迹 $\tau$ 指的是状态和动作的序列，轨迹常常也被称作**回合(episodes)**。  
第一个状态 *S<sub>0</sub>* 是从开始状态分布中随机采样的  
**转态转换**是由环境的自然法则确定的，并且只依赖于最近的动作，它们可以是确定性的也可以是随机的，智能体的动作由策略确定。

### **奖励**

强化学习中，奖励函数 *R* 非常重要。它由当前状态、已经执行的动作和下一步的状态共同决定。  
一种回报是**有限视野无折扣回报（finite-horizon undiscounted return）**，指的是在一个固定窗口步数内获得的奖励之和。另一种回报是**无限视野折扣回报（infinite-horizon discounted return）**，指的是智能体曾经获得的全部奖励之和，但是奖励会因为获得的时间不同而衰减。

## 强化学习算法概述

点击[这里](./../Reference/RLPaperSummary.md)查看算法原文

### **基于值函数的深度强化学习**

1. 深度 Q 网络

      - 深度双 Q 网络
      - 基于优势学习的深度 Q 网络
      - 基于优先级采样的深度 Q 网络
  
2. DQN 模型

      - 基于竞争架构的 DQN
      - 深度循环 Q 网络

### **基于策略梯度的深度强化学习**

1. 基于行动者评论家的深度策略梯度方法
2. 异步的优势行动者评论家算法

### **基于搜索与监督的深度强化学习**

1. 结合深度神经网络和 MCTS 的算法

### **分层深度强化学习**

1. 基于时空抽象和内在激励的分层深度强化学习
2. 基于内部 Option 的分层深度强化学习
3. 深度后续强化学习

### **多任务迁移深度强化学习**

1. 行为模拟的多任务迁移深度强化学习
2. 基于策略蒸馏的多任务迁移深度强化学习方法
3. 基于渐进式神经网络的迁移深度强化学习

### **多 agent 深度强化学习**

1. 深度强化学习中多 agent 的合作与竞争
2. 基于通信协议的分布式深度循环 Q 网络

### **基于记忆与推理的深度强化学习**

1. 基于记忆网络的深度强化学习模型
2. 模型无关的情节式控制器

### **深度强化学习中的探索与利用**

1. 利用深度预测模型来激励探索
2. 通过引导型 DQN 进行深度探索
3. 基于状态“伪”访问次数的内在激励

## 强化学习的应用

在深度学习已经取得了很大的进步的基础上，深度强化学习真正的发展归功于神经网络、深度学习以及计算力的提升，David就是使用了神经网络逼近值函数后提出深度强化学习（Deep Reinforcement Learning，DRL），并证明了确定性策略等。纵观近四年的ICML，NPIS等顶级会议论文，强化学习的理论进步，应用领域逐渐爆发式增广，目前已经在如下领域有了广泛使用:

- 自动驾驶：自动驾驶载具（self-driving vehicle）
- 控制论(离散和连续大动作空间): 玩具直升机、Gymm_cotrol物理部件控制、机器人行走、机械臂控制
- 游戏和博弈论：Go, Atari 2600, StarCraft II等
- 自然语言处理：机器翻译, 文本序列预测，问答系统，人机对话
- 超参数学习：神经网络参数自动设计
- 推荐系统：阿里巴巴黄皮书（商品推荐），广告投放。
- 智能电网：电网负荷调试，调度等
- 通信网络：动态路由, 流量分配等
- 财务与财经系统分析与管理
- 智能医疗
- 智能交通网络及网络流
- 物理化学实验：定量实验，核素碰撞，粒子束流调试等
- 程序学习和网络安全：网络攻防等

## 强化学习发展简史

强化学习的早期历史有两个主线，悠久和丰富，都是在现代强化学习交织之前独立进行的。一个主线涉及通过试错试验来学习，并且起源于动物学习的心理学。这个主线贯穿了人工智能领域的一些最早的工作，并导致了20世纪80年代早期强化学习的复兴。第二个主线涉及使用值函数和动态规划的最优控制问题及其解决方案。在大多数情况下，这个主线不涉及学习。这两个主线大多是独立的，但在某种程度上相互关联，还有就是围绕着时序差分方法的第三个不那么明显的线索。所有这三个主线在20世纪80年代后期汇集在一起，产生了现代的强化学习领域。

### **时间线**

1. 1954年Minsky首次提出“强化”和“强化学习”的概念和术语。
2. 1957年，Bellman提出了求解最优控制问题以及最优控制问题的随机离散版本马尔可夫决策过程（Markov Decision Process，MDP）的动态规划（Dynamic Programming）方法，而该方法的求解采用了类似强化学习试错迭代求解的机制。尽管他只是采用了强化学习的思想求解马尔可夫决策过程，但事实上却导致了马尔可夫决策过程成为定义强化学习问题的最普遍形式，加上其方法的现实操作性，以致后来的很多研究者都认为强化学习起源于Bellman的动态规划，随后Howard提出了求解马尔可夫决策过程的策略迭代方法。经过沉寂了了一段时间后。
3. 1965年在控制理论中Waltz和傅京孙也提出这一概念，描述通过奖惩的手段进行学习的基本思想。他们都明确了“试错”是强化学习的核心机制。
4. 1977年Werbos提出只适应动态规划算法。
5. 1988年sutton提出时间差分算法。
6. 1989年，Watkins提出的Q学习进一步拓展了强化学习的应用和完备了强化学习。Q学习使得在缺乏立即回报函数（仍然需要知道最终回报或者目标状态）和状态转换函数的知识下依然可以求出最优动作策略，换句话说，Q学习使得强化学习不再依赖于问题模型。此外Watkins还证明了当系统是确定性的马尔可夫决策过程，并且回报是有限的情况下，强化学习是收敛的，也即一定可以求出最优解。至今，Q学习已经成为最广泛使用的强化学习方法。
7. 1994年rummery 提出Saras算法。
8. 1996年Bersekas提出解决随机过程中优化控制的神经动态规划方法。
9. 2006年Kocsis提出了置信上限树算法。
10. 2009年kewis提出反馈控制只适应动态规划算法。
11. 2013年，DeepMind发表了利用强化学习玩Atari游戏的论文。
12. 2014年silver提出确定性策略梯度（Policy Gradents）算法。
13. 2015年Google-deepmind 提出Deep-Q-Network算法。
14. 2015年10月，由Google DeepMind公司开发的AlphaGo程序击败了人类高级选手樊麾，成为第一个无需让子即可在19路棋盘上击败围棋职业棋手的计算机围棋程序，并写进了历史，论文发表在国际顶级期刊《Science》上。
15. 2016年3月，透过自我对弈数以万计盘进行练习强化，AlphaGo在一场五番棋比赛中4:1击败顶尖职业棋手李世石。
16. 2016年12月，Master(AlphaGo版本)出现于弈城围棋网和腾讯野狐围棋网，取得60连胜的成绩，以其空前的实力轰动了围棋界。
17. 2019年4月，OpenAI Five 成为了首个战胜了世界冠军战队的 AI 系统，胜率高达99.4%。
18. 2019年8月，DeepMind在Nature上发表论文，基于多智能体强化学习算法的AlphaStar已经《星际争霸II》游戏中达到特级大师水平
19. 2021年2月，李飞飞团队提出了一个新的计算框架——深度进化强化学习——Deep Evolutionary Reinforcement Learning (DERL)，基于该框架，具身智能体可以在多个复杂环境中执行多个任务。

## 参考

- [1] [维基百科]((https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0))
- [2] [深度强化学习实验室](http://www.neurondance.com/)
- [3] [深度强化学习发展史——知乎](https://zhuanlan.zhihu.com/p/56399184)
- [4] [Reinforcement Learning 一：历史发展背景与介绍——CSDN](https://blog.csdn.net/qq_20499063/article/details/78762596)
- [5] [深度强化学习 Spinning Up 项目中文版](https://spinningup.qiwihui.com/zh_CN/latest/)
- [6] [深度强化学习综述](http://cjc.ict.ac.cn/online/cre/lq-2017119103322.pdf)
- [7] [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)
- [8] [Dota 2 with Large Scale Deep Reinforcement Learning](https://cdn.openai.com/dota-2.pdf)
- [9] [Grandmaster level in StarCraft II using multi-agent reinforcement learning](https://www.nature.com/articles/s41586-019-1724-z)
- [10] [Embodied Intelligence via Learning and Evolution](https://arxiv.org/pdf/2102.02202.pdf)
